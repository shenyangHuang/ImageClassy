{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy   as np \n",
    "import scipy.misc # to visualize only  \n",
    "\n",
    "#x = np.loadtxt(\"data/train_x.csv\", delimiter=\",\") # load from text \n",
    "#y = np.loadtxt(\"data/train_y.csv\", delimiter=\",\") \n",
    "#x = x.reshape(-1, 64, 64) # reshape \n",
    "#y = y.reshape(-1, 1)\n",
    "np.save(\"data/\"+\"X_train\",x)\n",
    "np.save(\"data/\"+\"y_train\",y)\n",
    "print(\"X.shape\",x.shape, \"Y.shape\",y.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.load(\"data/X_train.npy\")\n",
    "y = np.load(\"data/y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#using 1 hot encoding encode the 40 unique classes of y \n",
    "decode = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 24, 25, 27, 28, 30, 32, 35, 36, 40, 42, 45, 48, 49, 54, 56, 63, 64, 72, 81]\n",
    "encodedY = np.zeros((50000,40))\n",
    "for i in range(0,49999):\n",
    "    index = decode.index(y[i])\n",
    "    encodedY[i][index] = 1\n",
    "\n",
    "flatX = np.zeros((50000,4096))\n",
    "for i in range(0,49999):\n",
    "    flatX[i] = x[i].flatten()\n",
    "    #normalize input data\n",
    "    mean = np.mean(flatX[i])\n",
    "    flatX[i] = (flatX[i]- mean)/128.0\n",
    "    \n",
    "#for early testing, use 8% as training set and 2% as testing set\n",
    "inputX = flatX[0:800]\n",
    "validX = flatX[800:1000]\n",
    "inputY = encodedY[0:800]\n",
    "validY = encodedY[800:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12503703  0.1245883   0.12599021  0.12528974  0.12496891  0.12422922\n",
      "   0.12596939  0.12434601]\n",
      " [ 0.12389137  0.12314523  0.12452901  0.12394216  0.12363384  0.12292305\n",
      "   0.12461102  0.12296018]\n",
      " [ 0.12413226  0.12338768  0.12478417  0.12406249  0.12374302  0.1230027\n",
      "   0.12474091  0.12342825]\n",
      " [ 0.1269825   0.12654858  0.12765206  0.12693952  0.12659277  0.1258398\n",
      "   0.12760747  0.12628544]\n",
      " [ 0.12570119  0.12528005  0.12668793  0.12558016  0.12521641  0.12444327\n",
      "   0.12624398  0.12496471]\n",
      " [ 0.12461189  0.12413082  0.12551964  0.1250849   0.12452318  0.12385781\n",
      "   0.12546443  0.12404205]\n",
      " [ 0.12293163  0.12247739  0.123848    0.12341202  0.12312072  0.1221537\n",
      "   0.12377098  0.12237054]\n",
      " [ 0.12672024  0.12625047  0.12766219  0.12702548  0.12672892  0.12600619\n",
      "   0.12741241  0.12605292]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "class FeedForwardNeuralNetwork():\n",
    "    #Nlayer must be >=2\n",
    "    def __init__(self,NLayer,Nneuron,ranSeed,Ninput,Noutput,bias):\n",
    "        #specify the number of hidden layers , number of neurons needed per layer, number of weights per neuron\n",
    "        #ranSeed specify the random seed used in initializing weights\n",
    "        #activation specified what activation function is to be used, pass in 0 to use default sigmoid function\n",
    "        #-------------------------------\n",
    "        np.random.seed(ranSeed)\n",
    "        self.bias = bias\n",
    "        self.Nneuron = Nneuron\n",
    "        self.Ninput = Ninput\n",
    "        self.Noutput = Noutput\n",
    "        self.NLayer = NLayer\n",
    "        #input weights\n",
    "        self.InputWeight = np.random.uniform(0,1,(Nneuron,Ninput))\n",
    "        #this is weight for neurons from hidden layer 1 to (n-1)\n",
    "        self.neurons = np.random.uniform(0,1,(NLayer-1,Nneuron,Nneuron))\n",
    "        #store output of each layer\n",
    "        self.O = np.random.uniform(0,1,(NLayer,Nneuron))\n",
    "        #the weight for the last layer of neurons\n",
    "        self.OutputWeight = np.random.uniform(0,1,(Nneuron,Noutput))\n",
    "        #encoding output layer with multi-class domains, each output neuron represents a discrete output value\n",
    "        #see as an classification task\n",
    "        self.output = np.random.uniform(0,1,Noutput)\n",
    "    \n",
    "    #allows users to manually set all the weights\n",
    "    def set_Weights(self,InputWeights,OutputWeights,HiddenWeights):\n",
    "        self.OutputWeight = OutputWeights\n",
    "        self.InputWeight = InputWeights\n",
    "        self.neurons = HiddenWeights\n",
    "    \n",
    "    \n",
    "    \n",
    "    #default activation function, normalize input into 0 and 1\n",
    "    # normalise them between 0 and 1.\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    #train the neural network\n",
    "    #can change learning rate\n",
    "    def train(self, num_training_example, training_inputs, training_outputs, num_iteration, learning_rate):\n",
    "        #use to store outputs\n",
    "        outputArr = np.zeros((num_training_example,self.Noutput))\n",
    "        for iteration in xrange(num_iteration):\n",
    "            for i in range(0,num_training_example):\n",
    "                #---------------------------------------------\n",
    "                #need a output layer special gamma\n",
    "                #-------F--------------------------------------\n",
    "                gammaOutput = np.zeros(self.Noutput)\n",
    "                # gamma is the error correction for all neurons in NN\n",
    "                # gamma values are used to backpropagate\n",
    "                gamma = np.zeros((self.NLayer,self.Nneuron))\n",
    "                #---------------------------------------------\n",
    "                # Forward pass\n",
    "                self.forward(training_inputs[i])\n",
    "                outputArr[i] = np.copy(self.output)\n",
    "                #---------------------------------------------\n",
    "                #compute corrections\n",
    "                # first calculate correction in the output layer\n",
    "                for k in range(0,self.Noutput):\n",
    "                    gammaOutput[k] = self.sigmoid_derivative(self.output[k])*(training_outputs[i][k]-self.output[k])\n",
    "                    #calculate the gamma values for the last hidden layer\n",
    "                    #And update weights for output layer\n",
    "                    for z in range(0,self.Nneuron):\n",
    "                        gamma[self.NLayer-1][z] = self.sigmoid_derivative(self.O[self.NLayer-1][z])*dot(gammaOutput,self.OutputWeight[z])\n",
    "                for z in range(0,self.Nneuron):\n",
    "                    for k in range(0,self.Noutput):\n",
    "                        #print (\"position is \" + str(z) + str(k))\n",
    "                        #print (self.OutputWeight[z][k])\n",
    "                        self.OutputWeight[z][k] = self.OutputWeight[z][k] + (learning_rate*gammaOutput[k]*\n",
    "                                                                              self.O[self.NLayer-1][z])\n",
    "                        #print (self.OutputWeight[z][k])\n",
    "                        #print ((learning_rate*gammaOutput[k]*self.O[self.NLayer-1][z]))\n",
    "                #--------------------------------------\n",
    "                #calculate the backprop for hidden layer\n",
    "                #j is counting the current layer\n",
    "                #z is counting the neuron position\n",
    "                #propagate gamma values\n",
    "                for j in range(self.NLayer-2,-1,-1):\n",
    "                    for z in range(0,self.Nneuron):\n",
    "                        #multiple the derivative with all the connected neuron's gamma                \n",
    "                        #SumGamma will compute all the propagated gammas\n",
    "                        gamma[j][z] = self.sigmoid_derivative(self.O[j][z])*(self.SumGamma(gamma,j,z))\n",
    "                        #update the weights now\n",
    "                        for k in range(0,self.Nneuron):\n",
    "                            self.neurons[j][z][k] = self.neurons[j][z][k] + (learning_rate*gamma[j][z]*\n",
    "                            self.O[j][k])\n",
    "                #---------------------------------------\n",
    "        print (outputArr)\n",
    "                \n",
    "        \n",
    "    #compute the propagated gamma of a neuron\n",
    "    def SumGamma(self,gamma,Layer,position):\n",
    "        output = dot(gamma[Layer+1],self.neurons[Layer][position])\n",
    "        return output\n",
    "                \n",
    "    # pass input through all the layers until the output layer\n",
    "    def forward(self, inputs):\n",
    "        #first pass from input layer to hidden layer\n",
    "        current = np.zeros(self.Nneuron)\n",
    "        for i in range(0,self.Nneuron):\n",
    "            current[i] = self.sigmoid(dot(inputs,self.InputWeight[i])+self.bias)\n",
    "            self.O[0][i] = current[i]\n",
    "        #passing through hidden layers\n",
    "        for i in range(0,self.NLayer-1):\n",
    "            for j in range(0,self.Nneuron):\n",
    "                current[j] = self.sigmoid(dot(self.O[i],self.neurons[i][j])+self.bias)\n",
    "            self.O[i+1] = np.copy(current)\n",
    "        #now from hidden layer to output layer\n",
    "        #the weight for the last hidden layer is the weight for the output layer \n",
    "        #---------------------------------------------------------------\n",
    "        for i in range(0,self.Noutput):\n",
    "            self.output[i]= self.sigmoid(dot(current,self.OutputWeight[:,i])+self.bias)\n",
    "        #----------------------------------------------------------------\n",
    "        \n",
    "    #a simple function to see the validation accuracy    \n",
    "    def validate(self, num_validate_example, validating_inputs, validating_outputs):\n",
    "        count = 0\n",
    "        for i in range(0,num_validate_example):\n",
    "            self.forward(validating_inputs[i])\n",
    "            if(self.output.index(1) == validating_outputs[i].index(1)):\n",
    "                count = count + 1\n",
    "        print(\"accuracy is\" + str(count*1.0/num_validate_example))\n",
    "    \n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Intialise a single neuron neural network.\n",
    "    #def __init__(self,NLayer,Nneuron,ranSeed,Ninput,Noutput,bias):\n",
    "    #neural_network = FeedForwardNeuralNetwork(3,200,1,4096,40,1)\n",
    "    neural_network = FeedForwardNeuralNetwork(2,3,1,8,8,1)\n",
    "    #neural_network.set_Weights(InputWeights,OutputWeights,HiddenWeights)\n",
    "    trainX = [[1,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,1],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0],[0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0]]\n",
    "    trainY = [[1,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,1],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0],[0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0]]\n",
    "    neural_network.train(8,trainX,trainY,10000,0.01)\n",
    "    #def train(self, num_training_example, training_inputs, training_outputs, num_iteration, learning_rate):\n",
    "    #neural_network.train(800,inputX,inputY,20,0.001)\n",
    "    #def validate(self, num_validate_example, validating_inputs, validating_outputs):\n",
    "    #neural_network.validate(200,validX,validY)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
