{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy   as np \n",
    "x = np.loadtxt(\"data/train_x.csv\", delimiter=\",\") # load from text \n",
    "y = np.loadtxt(\"data/train_y.csv\", delimiter=\",\") \n",
    "x = x.reshape(-1, 64, 64) # reshape \n",
    "y = y.reshape(-1, 1)\n",
    "np.save(\"data/\"+\"X_train\",x)\n",
    "np.save(\"data/\"+\"y_train\",y)\n",
    "print(\"X.shape\",x.shape, \"Y.shape\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.load(\"data/X_train.npy\")\n",
    "y = np.load(\"data/y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#using 1 hot encoding encode the 40 unique classes of y \n",
    "decode = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 24, 25, 27, 28, 30, 32, 35, 36, 40, 42, 45, 48, 49, 54, 56, 63, 64, 72, 81]\n",
    "encodedY = np.zeros((50000,40))\n",
    "for i in range(0,49999):\n",
    "    index = decode.index(y[i])\n",
    "    encodedY[i][index] = 1\n",
    "\n",
    "#flat out X from(64,64) to 4096\n",
    "#and normalize the input\n",
    "flatX = np.zeros((50000,4096))\n",
    "for i in range(0,49999):\n",
    "    flatX[i] = x[i].flatten()\n",
    "    #normalize input data\n",
    "    mean = np.mean(flatX[i])\n",
    "    flatX[i] = (flatX[i]- mean)/128.0\n",
    "    \n",
    "    \n",
    "#shuffle    \n",
    "np.random.shuffle(flatX)\n",
    "np.random.shuffle(encodedY)\n",
    "\n",
    "#take 90% training set and 10% validation set\n",
    "train_X = flatX[0:45000]\n",
    "Valid_X = flatX[45000:50000]\n",
    "train_Y = encodedY[0:45000]\n",
    "Valid_X = encodedY[45000:50000]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, array, random, dot\n",
    "from random import shuffle\n",
    "\n",
    "class FeedForwardNeuralNetwork():\n",
    "    #Nlayer must be >=2\n",
    "    def __init__(self,NLayer,Nneuron,ranSeed,Ninput,Noutput,bias):\n",
    "        #######################################################################################################\n",
    "        #specify the number of hidden layers , number of neurons needed per layer, number of weights per neuron\n",
    "        #ranSeed specify the random seed used in initializing weights\n",
    "        #activation specified what activation function is to be used, pass in 0 to use default sigmoid function\n",
    "        #######################################################################################################\n",
    "        np.random.seed(ranSeed)\n",
    "        self.bias = [bias]*NLayer\n",
    "        self.Nneuron = Nneuron\n",
    "        self.Ninput = Ninput\n",
    "        self.Noutput = Noutput\n",
    "        self.NLayer = NLayer\n",
    "        #######################################################################################################\n",
    "        #input weights, hidden weights and output weights\n",
    "        self.InputWeight = np.random.uniform(0,1,(Nneuron,Ninput))\n",
    "        self.HiddenWeight = np.random.uniform(0,1,(NLayer-1,Nneuron,Nneuron))\n",
    "        self.OutputWeight = np.random.uniform(0,1,(Nneuron,Noutput))\n",
    "        ######################################################################################################\n",
    "    \n",
    "    #####################################################################\n",
    "    #allows users to manually set all the weights\n",
    "    def set_Weights(self,InputWeights,OutputWeights,HiddenWeights):\n",
    "        self.OutputWeight = OutputWeights\n",
    "        self.InputWeight = InputWeights\n",
    "        self.HiddenWeight = HiddenWeights\n",
    "        \n",
    "    #####################################################################\n",
    "    #default activation function, normalize input into 0 and 1\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "    \n",
    "    #####################################################################\n",
    "    #The derivative of the Sigmoid function.\n",
    "    #This is the gradient of the Sigmoid curve.\n",
    "    def sigmoid_derivative(self, sigmoid):\n",
    "        return (sigmoid) * (1 - (sigmoid))\n",
    "    \n",
    "    #####################################################################\n",
    "    def softmax(self,inputs):\n",
    "        f_i = inputs\n",
    "        log_c = -np.max(f_i)  \n",
    "        sum_j = np.sum(np.exp(f_i)+log_c)\n",
    "        out = np.exp(f_i) / sum_j\n",
    "        return out\n",
    "    \n",
    "    #####################################################################\n",
    "    def cross_entropy_loss(self,px,y):\n",
    "        return (-y*(np.log(px)))\n",
    "\n",
    "    ####################################################################\n",
    "    #derivative of softmax function\n",
    "    def softmax_derivative(self,target,prediction,O):\n",
    "        '''\n",
    "        input:\n",
    "        target: one hot encoded target output\n",
    "        prediction: one hot encoded predictions\n",
    "        O: inputs from hidden layers\n",
    "        \n",
    "        output:\n",
    "        dW: same shape as outputWeight (Nneuron,Noutput)\n",
    "        '''\n",
    "        #find the target class\n",
    "        for i in range(0,len(target)):\n",
    "            if(target[i] == 1):\n",
    "                y=i\n",
    "        num_classes = len(prediction)\n",
    "        dW = np.zeros((self.Nneuron,self.Noutput))\n",
    "        for j in range(num_classes):\n",
    "            dW[:,j] = (prediction[j]-(j == y)) * O[(self.NLayer)-1]\n",
    "        return dW\n",
    "        \n",
    "    #####################################################################\n",
    "    def softmax_delta(self,prediction,target,dW):\n",
    "        out = np.zeros(self.Noutput)\n",
    "        for i in range(0,self.Noutput):\n",
    "            out[i] = dot(self.OutputWeight[:,i],dW[:,i])\n",
    "        return out\n",
    "    \n",
    "    #####################################################################\n",
    "    #use to predict output for one hot encoding, largest probability is the output 1, others are 0\n",
    "    def where(self,inputs):\n",
    "        return np.where(inputs==inputs.max())\n",
    "    \n",
    "    #####################################################################\n",
    "    #take input a 2d array consists of answers from each sample output\n",
    "    def Decode(self,inputs):\n",
    "        Decoded = np.zeros((len(inputs),len(inputs[0])))\n",
    "        for i in range(len(inputs)):\n",
    "            x = np.where( inputs[i]==max(inputs[i]))\n",
    "            Decoded[i][x] = 1\n",
    "        return Decoded\n",
    "    \n",
    "    \n",
    "    #####################################################################\n",
    "    #train the neural network\n",
    "    '''\n",
    "     Inputs:\n",
    "      - num_training_example : int \n",
    "      - training_inputs: [num_training_example][each_input]\n",
    "      - training_outputs: [num_training_example][each_output]\n",
    "      - num_iteration: How many times you use the data to train NN\n",
    "      - learning rate: recommend 0.001\n",
    "    '''\n",
    "    def train(self, num_training_example, training_inputs, training_outputs, num_iteration, learning_rate):\n",
    "        #use to store outputs\n",
    "        outputArr = np.zeros((num_training_example,self.Noutput))\n",
    "        \n",
    "        for iteration in xrange(num_iteration):\n",
    "            print(\"current round is\" + \" \"+ str(iteration))\n",
    "            for i in range(0,num_training_example):\n",
    "                #------------------------------------------------\n",
    "                # delta is the error correction for all neurons in NN\n",
    "                # delta values are used to backpropagate\n",
    "                delta = np.zeros((self.NLayer,self.Nneuron))\n",
    "                deltaOutput = np.zeros(self.Noutput)\n",
    "                #---------------------------------------------\n",
    "                # Forward pass\n",
    "                O,prediction = self.forward(training_inputs[i])\n",
    "                #---------------------------------------------\n",
    "                #compute corrections\n",
    "                # first calculate correction in the output layer\n",
    "                #----------------------------------------------------\n",
    "                #update weights for output layer\n",
    "                Woutput_Update = self.softmax_derivative(training_outputs[i],prediction,O)\n",
    "                deltaOutput = self.softmax_delta(prediction,training_outputs[i],Woutput_Update)\n",
    "                #########################################################\n",
    "                for z in range(0,self.Nneuron):\n",
    "                    for k in range(0,self.Noutput):\n",
    "                        self.OutputWeight[z][k] = self.OutputWeight[z][k] + (learning_rate*Woutput_Update[z][k])\n",
    "                #---------------------------------------------------  \n",
    "                for z in range(0,self.Nneuron):\n",
    "                    #calculate the delta values for the last hidden layer\n",
    "                    delta[self.NLayer-2][z] = self.sigmoid_derivative(O[self.NLayer-1][z])*dot(deltaOutput,self.OutputWeight[z])\n",
    "                \n",
    "                #And update weights for bias in output layer\n",
    "                self.bias[self.NLayer-1] = self.bias[self.NLayer-1] + (learning_rate*Woutput_Update[z][0])\n",
    "                #update bias for hidden layers and input layer\n",
    "                for z in range(0,self.NLayer-1):\n",
    "                    self.bias[z] = self.bias[z] + (learning_rate*Woutput_Update[z][0])        \n",
    "                #--------------------------------------\n",
    "                #calculate the backprop for hidden layer\n",
    "                #j is counting the current layer\n",
    "                #z is counting the neuron position\n",
    "                #propagate delta values\n",
    "                for j in range(self.NLayer-2,-1,-1):\n",
    "                    for z in range(0,self.Nneuron):\n",
    "                        #multiple the derivative with all the connected neuron's delta            \n",
    "                        #Sumdelta will compute all the propagated deltas\n",
    "                        delta[j][z] = self.sigmoid_derivative(O[j][z])*(self.SumDelta(delta,j,z))\n",
    "                        #update the weights now\n",
    "                        for k in range(0,self.Nneuron):\n",
    "                            self.HiddenWeight[j][z][k] = self.HiddenWeight[j][z][k] + (learning_rate*delta[j][z]*\n",
    "                            O[j][k])\n",
    "                #---------------------------------------\n",
    "        \n",
    "    #compute the propagated delta of a neuron\n",
    "    def SumDelta(self,delta,Layer,position):\n",
    "        output = dot(delta[Layer+1],self.HiddenWeight[Layer][position])\n",
    "        return output\n",
    "                \n",
    "    # pass input through all the layers until the output layer\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        input: \n",
    "        inputs from the input layer(shape: self.Ninput)\n",
    "        output: \n",
    "        O: intermediate output from all the hidden layers (shape: (self.NLayer,self.Nneuron))\n",
    "        Prediction : values in output layer after softmax activation (shape: (self.Noutput,)\n",
    "        '''\n",
    "        #O are results within NN and output is obtained from the output layer\n",
    "        #encoding output layer with multi-class domains, each output neuron represents a discrete output value\n",
    "        O = np.zeros((self.NLayer,self.Nneuron))\n",
    "        prediction = np.zeros(self.Noutput)\n",
    "        ########################################################\n",
    "        #first pass from input layer to hidden layer\n",
    "        current = np.zeros(self.Nneuron)\n",
    "        for i in range(0,self.Nneuron):\n",
    "            current[i] = self.sigmoid(dot(inputs,self.InputWeight[i])+self.bias[0])\n",
    "        O[0] = np.copy(current)\n",
    "        ########################################################\n",
    "        #passing through hidden layers\n",
    "        #use input from last layer as input\n",
    "        for i in range(0,self.NLayer-1):\n",
    "            for j in range(0,self.Nneuron):\n",
    "                current[j] = self.sigmoid(dot(O[i],self.HiddenWeight[i][j])+self.bias[i])\n",
    "            O[i+1] = np.copy(current)\n",
    "        #now from hidden layer to output layer\n",
    "        #the weight for the last hidden layer is the weight for the output layer\n",
    "        #---------------------------------------------------------------\n",
    "        Out = []\n",
    "        for i in range(0,self.Noutput):\n",
    "            Out.append(dot(current,self.OutputWeight[:,i])+self.bias[self.NLayer-1])\n",
    "        #----------------------------------------------------------------\n",
    "        prediction = np.copy(self.softmax(Out))\n",
    "        return (O,prediction)\n",
    "        \n",
    "    ##########################################################    \n",
    "    #a simple function to check the validation accuracy    \n",
    "    def validate(self, num_validate_example, validating_inputs, validating_outputs):\n",
    "        count = 0\n",
    "        for i in range(0,num_validate_example):\n",
    "            O,prediction = self.forward(validating_inputs[i])\n",
    "            index = self.where(prediction)\n",
    "            if( index == self.where(validating_outputs[i])):\n",
    "                count = count + 1\n",
    "        print(\"accuracy is\" + str(count*1.0/num_validate_example))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run code from here\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Intialise a feedforward neural network\n",
    "    #def __init__(self,NLayer,Nneuron,ranSeed,Ninput,Noutput,bias):\n",
    "    neural_network = FeedForwardNeuralNetwork(4,200,1,4096,40,1)\n",
    "    #def train(self, num_training_example, training_inputs, training_outputs, num_iteration, learning_rate):\n",
    "    neural_network.train(45000,train_X,train_Y,100,0.001)\n",
    "    #def validate(self, num_validate_example, validating_inputs, validating_outputs):\n",
    "    neural_network.validate(5000,valid_X,valid_Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
